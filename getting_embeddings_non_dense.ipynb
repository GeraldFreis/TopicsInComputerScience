{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"IMDB_dataset_final.csv\", low_memory=False)\n",
    "\n",
    "# printing statements to check the variables\n",
    "raw_x = raw_data.review; # print(raw_x)\n",
    "raw_y = raw_data.sentiment; # print(raw_y)\n",
    "\n",
    "number_of_words_in_dic = 37500; \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_words_in_dic) # to tokenize the words for learning\n",
    "tokenizer.fit_on_texts(raw_x)\n",
    "tokenized_sentiments = tokenizer.texts_to_sequences(raw_x) # converting the words to number arrays\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "padded_tokenized_sentiments= tf.keras.utils.pad_sequences(tokenized_sentiments, padding=\"post\", maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential # we will be using this for the CNN\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code beneath changes every time you run it and hence shows a bad approach to extracting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 50)          1875000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 998, 32)           4832      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                330       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,880,173\n",
      "Trainable params: 1,880,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# importing the trained keras cnn\n",
    "CNN = tf.keras.models.load_model(\"CNN_Non_Dense\")\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the node embedding outputs from the STABLE CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04275493 -0.08479704  0.0292923  ...  0.08790416  0.07556851\n",
      "  -0.06879054]\n",
      " [ 0.00583083 -0.01126311  0.00234818 ...  0.08790416  0.07556851\n",
      "  -0.06879054]\n",
      " [ 0.01972886  0.07676113 -0.08007895 ...  0.08790416  0.07556851\n",
      "  -0.06879054]\n",
      " ...\n",
      " [ 0.01972886  0.07676113 -0.08007895 ...  0.08790416  0.07556851\n",
      "  -0.06879054]\n",
      " [ 0.08189929  0.08689225  0.11288299 ...  0.08790416  0.07556851\n",
      "  -0.06879054]\n",
      " [-0.09420051  0.00115355 -0.06794097 ...  0.08790416  0.07556851\n",
      "  -0.06879054]], shape=(50000, 50000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# getting the outputs from the embedding layer of the CNN\n",
    "embeddings = keras.Model(inputs=CNN.inputs, outputs=CNN.get_layer(name=\"embedding\").output) \n",
    "\n",
    "embedding_raw_outputs = embeddings(padded_tokenized_sentiments[0:50000:1])\n",
    "flatten_layer = keras.layers.Flatten()\n",
    "embedding_outputs = flatten_layer(embedding_raw_outputs)\n",
    "print(embedding_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting this big tensor up into train test and validation segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([37314, 20848, 16191, ...,  6215,  4695, 42419])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m train_x, test_x, train_y, test_y \u001b[39m=\u001b[39m train_test_split(embedding_outputs, raw_y, random_state\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2640\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2636\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2638\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m-> 2640\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m   2641\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[0;32m   2642\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[0;32m   2643\u001b[0m     )\n\u001b[0;32m   2644\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2642\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2636\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2638\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m   2640\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2641\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2642\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[0;32m   2643\u001b[0m     )\n\u001b[0;32m   2644\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\sklearn\\utils\\__init__.py:355\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m    354\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    356\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\sklearn\\utils\\__init__.py:184\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    183\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m--> 184\u001b[0m \u001b[39mreturn\u001b[39;00m array[key] \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m array[:, key]\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:906\u001b[0m, in \u001b[0;36m_check_index\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    901\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(idx, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    902\u001b[0m \u001b[39mif\u001b[39;00m (dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m dtypes\u001b[39m.\u001b[39mas_dtype(dtype) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _SUPPORTED_SLICE_DTYPES \u001b[39mor\u001b[39;00m\n\u001b[0;32m    903\u001b[0m     idx\u001b[39m.\u001b[39mshape \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(idx\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    904\u001b[0m   \u001b[39m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[39;00m\n\u001b[0;32m    905\u001b[0m   \u001b[39m# will break `_slice_helper` contract.\u001b[39;00m\n\u001b[1;32m--> 906\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(_SLICE_TYPE_ERROR \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(idx))\n",
      "\u001b[1;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([37314, 20848, 16191, ...,  6215,  4695, 42419])"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(embedding_outputs, raw_y, random_state=1000, shuffle=True, test_size=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
